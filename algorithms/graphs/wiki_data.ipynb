{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Graph Data from Wikipedia\"\n",
    "date: 2021-02-10\n",
    "type: technical_note\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Network from Unclean Data\n",
    "\n",
    "I'm working through [Complex Network Analysis in Python](https://smile.amazon.com/Complex-Network-Analysis-Python-Recognize/dp/1680502697?sa-no-redirect=1) and one of the earlier examples is just too good to pass up, wherein the author generates useful Network data by combing through links from a starting Wikipedia page.\n",
    "\n",
    "This technique is called \"Snowball Sampling\" and entails starting from some *seed*, or starting point, and running the algorithm which \"uses retrieved data to find *more* data,\" hence *snowball.* It does this by executing a Breadth-First Search from the starting seed, with a few thoughtful checks and data cleaning steps that I'll go through in the next section.\n",
    "\n",
    "## In the Code\n",
    "\n",
    "The code, [copied from the book repository here](https://github.com/NapsterInBlue/complex-network-analysis-python/blob/master/book_files/wiki2net.py), looks pretty dense, but is actually quite elegant. Pasting in its entirety, but segmenting to make observations on what's going on under the hood.\n",
    "\n",
    "### Core Objects\n",
    "\n",
    "After the relevant library imports, we start with our main objects:\n",
    "\n",
    "- `F`: the blank network object that will hold our data\n",
    "- `SEED`: the title of the starting page\n",
    "- `todo_set`: A `set` (not `list`, for better retrieval!) that holds all pages we still need to crawl\n",
    "- `done_set`: A `set` holding all pages we've already been to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import networkx as nx\n",
    "import wikipedia\n",
    "\n",
    "F = nx.DiGraph()\n",
    "\n",
    "SEED = \"Complex network\".title()\n",
    "\n",
    "todo_set = set(SEED)   # The SEED itself\n",
    "done_set = set()       # Nothing is done yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this next section, it's extremely important that we keep track of what layer we're currently operating on. Wikipedia is enormous, and if we're not careful, our scraper would just go down the \"one more link\" rabbit hole and spend hours, if not days/months scraping until we finish the site.\n",
    "\n",
    "Therefore, for each page we scrape, we want to catalog both its title **and** what layer away from our seed the node is. This gives us:\n",
    "\n",
    "- `todo_lst`: A `list` of `(layer, page_title)` `tuples`\n",
    "- `layer`: The current (integer) layer value, representing our distance from the seed\n",
    "- `page`: The current page name as a string\n",
    "\n",
    "We initialize `todo_lst` with the below because *we haven't yet begun scraping*. The first values of `layer` and `page` get updated accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "todo_lst = [(0, SEED)] # The SEED is in the layer 0\n",
    "\n",
    "layer, page = todo_lst[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Clever Conditional\n",
    "\n",
    "Specific to this particular use-case, the author also provided us with a list, `STOPS`, which represent links that are either of very little informational value, or show up on virtually every page and provide no real insight, into the network built around your `seed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPS = (\"International Standard Serial Number\",\n",
    "         \"International Standard Book Number\",\n",
    "         \"National Diet Library\",\n",
    "         \"International Standard Name Identifier\",\n",
    "         \"International Standard Book Number (Identifier)\",\n",
    "         \"Pubmed Identifier\", \"Pubmed Central\",\n",
    "         \"Digital Object Identifier\", \"Arxiv\",\n",
    "         \"Proc Natl Acad Sci Usa\", \"Bibcode\",\n",
    "         \"Library Of Congress Control Number\", \"Jstor\",\n",
    "         \"Doi (Identifier)\", \"Isbn (Identifier)\",\n",
    "         \"Pmid (Identifier)\", \"Arxiv (Identifier)\",\n",
    "         \"Bibcode (Identifier)\", \"Pmc (Identifier)\",\n",
    "         \"Issn (Identifier)\", \"S2Cid (Identifier)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Loop\n",
    "\n",
    "With all of these parts together, the following algorithm basically:\n",
    "\n",
    "- Removes the next `(layer, page_title)` pair from the `todo_lst`\n",
    "- Uses this to open up the Wikipedia article corresponding to the `page_title`\n",
    "- Finds all of the links the article contains while\n",
    "\n",
    "    - Filtering out any pages that appear in our `STOPS` list\n",
    "    - Incrementing the `layer` value to be `current_layer + 1`\n",
    "    - Adding the new pair to the end of our `todo_list`\n",
    "    \n",
    "This ordering is crucial, as it ensures that we scrape the pages in `layer` order (e.g. `0` before `1`s, all `1`s before any `2`s, etc) until the only pages left in our `todo_lst` are those with a higher `layer` value than we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13506 nodes, 24391 edges\n"
     ]
    }
   ],
   "source": [
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings('ignore')\n",
    "\n",
    "while layer < 2:\n",
    "    del todo_lst[0] #(1)\n",
    "    done_set.add(page)\n",
    "    # print(layer, page)\n",
    "\n",
    "    try: #(2)\n",
    "        wiki = wikipedia.page(page)\n",
    "    except:\n",
    "        layer, page = todo_lst[0]\n",
    "        # print(\"Could not load\", page)\n",
    "        continue\n",
    "\n",
    "    for link in wiki.links: #(3)\n",
    "        link = link.title()\n",
    "        if link not in STOPS and not link.startswith(\"List Of\"):\n",
    "            if link not in todo_set and link not in done_set:\n",
    "                todo_lst.append((layer + 1, link))\n",
    "                todo_set.add(link)\n",
    "            F.add_edge(page, link)\n",
    "\n",
    "    layer, page = todo_lst[0] #(4)\n",
    "print(\"{} nodes, {} edges\".format(len(F), nx.number_of_edges(F)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some More Clever Data Fixes\n",
    "\n",
    "The author also provides a nice chunk of code to resolve any naming inconsistencies between two pages of the same content (e.g. \"Complex Network\" vs the plural \"Complex Network**s**\").\n",
    "\n",
    "It does this by checking all pairwise combinations of page titles to see if they match the singular/plural of the other, collecting matches into a list, `duplicates`.\n",
    "\n",
    "Then, they make use of `nx.contracted_nodes()`, whose middle argument is an iterable of node-key pairs telling `networkx` \"treat these two nodes as *one node*\". Concretely, if we had a linear network (`A - B - C - D`) and called `nx.contracted_nodes()` with middle argument `[(B, C)]`, then the network would make a new node, `BC` that squashed the two together, giving us a new network `A - BC - D`. Finally, the `self_loops=False` argument ensures that we don't preserve the \"connection/relationship between `B` and `C`\" -- they're the same node and that self-referencing loop becomes redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.remove_edges_from(nx.selfloop_edges(F))\n",
    "duplicates = [(node, node + \"s\") for node in F if node + \"s\" in F]\n",
    "for dup in duplicates:\n",
    "    F = nx.contracted_nodes(F, *dup, self_loops=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, they check for multi-word titles that are space-delimited vs hyphen-delimited but represent the same content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = [(x, y) for x, y \n",
    "              in [(node, node.replace(\"-\", \" \")) for node in F]\n",
    "              if x != y and y in F]\n",
    "for dup in duplicates:\n",
    "    F = nx.contracted_nodes(F, *dup, self_loops=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line is a bit hand-wavy, but essentially any node that survives the deduping, gets stuck with a new `contraction` attribute that marks what node got deleted in the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contraction': {'Complex Networks': {}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.nodes['Complex Network']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so this line just goes through and zeroes out all `contraction` values, because it's a useless attribute in this instance, and makes exporting the network to a standard format a chore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(F, 0, \"contraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgraphing\n",
    "\n",
    "At this point, we have a ton of nodes, and even more edges connecting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13390 23223\n"
     ]
    }
   ],
   "source": [
    "print(len(F.nodes), len(F.edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But *do we need all of this data*?\n",
    "\n",
    "Of the 14k nodes we pulled in our dataset, nearly 10k of them only have degree 1 (one in-edge from a neighbor, not no out-edge connecting to any data point in our first two layers). This means that we can lop off nearly 70% of the data we collected, without losing nodes that provide any real interesting properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    10436\n",
       "2     1596\n",
       "3      605\n",
       "5      230\n",
       "4      184\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "node_degrees = pd.Series([degree for title, degree in F.degree()])\n",
    "node_degrees.value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author does this by defining a `core` set of nodes that all have degree `>= 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = [node for node, deg in dict(F.degree()).items() if deg >= 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then defines a new, final, graph `G`, derived from our original graph `F`, but only for the nodes of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.subgraph(F, core)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only does the node count go down considerably, we also automatically rid ourselves of edges connecting to nodes outside of our `core` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2954 12787\n"
     ]
    }
   ],
   "source": [
    "print(len(G.nodes), len(G.edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing\n",
    "\n",
    "I'm leaving this commented out, because I have no use for it in this notebook, but writing the graph object to memory is as easy as calling the function that writes the data to (one of many) standard graph data formats\n",
    "\n",
    "``` python\n",
    "nx.write_graphml(G, \"output.graphml\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Together\n",
    "\n",
    "Finally, we can use the `in_degree` values as a measure of \"how fundamental is this page, in the world of our starting seed?\". Anyone with even a cursory understanding of Graph/Network Analysis won't be surprised to see these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Graph (Discrete Mathematics)', 68),\n",
       " ('Vertex (Graph Theory)', 64),\n",
       " ('Directed Graph', 58),\n",
       " ('Social Network', 56),\n",
       " ('Network Theory', 53),\n",
       " ('Adjacency Matrix', 53),\n",
       " ('Degree (Graph Theory)', 53),\n",
       " ('Network Science', 51),\n",
       " ('Graph Drawing', 50),\n",
       " ('Edge (Graph Theory)', 50)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_indegree = sorted(dict(G.in_degree()).items(),\n",
    "                      reverse=True, key=itemgetter(1))[:100]\n",
    "\n",
    "top_indegree[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
