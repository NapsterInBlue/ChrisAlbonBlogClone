{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Forward Propagation\"\n",
    "date: 2018-08-10\n",
    "type: technical_note\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propogation in a Neural Network is just an extrapolation of how we worked with [Logistic Regression](https://napsterinblue.github.io/notes/#ml_regression), where the caluculation chain just looked like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/logit.PNG\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our equation before,\n",
    "\n",
    "$\\hat{y} = w^{T} X + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "was much simpler in the sense that:\n",
    "\n",
    "- `X` was an `n x m` vector (`n` features, `m` training examples)\n",
    "- This was matrix-multiplied by `w` an `n x 1` vector of weights (`n` because we want a weight per feature)\n",
    "- Then we broadcast-added `b`\n",
    "- Until we wound up with an `m x 1` vector of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Different Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we get into Neural Networks, with multiple-dimension matrix-multiplication to go from layer to layer, things can get pretty hairy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/dimensions.PNG' align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our **input layer** `X` is still `n x m`\n",
    "- Our **output layer** is still `m x 1`.\n",
    "- **Hidden/Activation layers** are the nodes organized vertically that represent intermediate calculations.\n",
    "  - The superscript represents which layer a node falls in\n",
    "  - The subscript is which particular node you're referencing\n",
    "- The **weights matricies** are the values that take you from one layer to the next via matrix multiplication.\n",
    "  - *PAY CAREFUL ATTENTION TO THE FACT that **W1 takes you from layer 1 to layer 2***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping the Dimensions Straight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always refer back to the fact that dot-producting two matricies along a central dimension cancels it out. For instance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cancelling.png\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, understanding which dimension your data should be in is an exercise in plugging all of the gaps to get you from `X` to `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting to `a2` means following the equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^{[2]} = W^{[1]}X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As far as dimensions go, we're looking at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `X`: `n x m`\n",
    "- `a1`: `4 x m`\n",
    "\n",
    "Subbing the *dimensions* in for the variables, we can start to fill in the gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(4, m) = (?, ??) (n, m)$\n",
    "\n",
    "because we know that we want `4` as the first value\n",
    "\n",
    "$(4, m) = (4, ??) (n, m)$\n",
    "\n",
    "we just need\n",
    "\n",
    "$(4, m) = (4, n) (n, m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$dim_{W} = (4, n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Generally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If layer `j` is `m`-dimensional and layer `j+1` is `n`-dimensional\n",
    "\n",
    "$W^{j} \\quad \\text{(which maps from j to j+1) has dimensionality} \\quad (n \\times m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following image (grabbed from *Computing a Neural Network's Output* in Week 3) is as busy as it is informative.\n",
    "\n",
    "It color-codes the same simple as above, highlighting the stacking approach to go from various vectors (e.g. `z[1]1, z[1]2, z[1]3, z[1]4`) to one large, unified matrix of values (`Z[1]`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/vectorizing.png\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so the process becomes 4 simple equations for one training example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z^{[1]} = W^{[1]} x + b^{[1]}$\n",
    "\n",
    "$a^{[1]} = sigmoid(z^{[1]})$\n",
    "\n",
    "$z^{[2]} = W^{[2]} a^{[1]} + b^{[2]}$\n",
    "\n",
    "$a^{[2]} = sigmoid(z^{[2]})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "z1 = np.dot(W1, x) + b1\n",
    "a1 = sigmoid(z1)\n",
    "z2 = np.dot(W2, a1) + b2\n",
    "a2 = sigmoid(z2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to extend to multiple training examples, you introduce a `(i)` notation, where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^{[1](i)}_{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "refers to the `2nd` node activation, in the `1st` hidden layer, of the `ith` training example. And propogating for each prediction involves a big `for` loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "for i in range(len(x)):\n",
    "    z1[i] = np.dot(W1, x[i]) + b1\n",
    "    a1[i] = sigmoid(z1[i])\n",
    "    z2[i] = np.dot(W2, a1[i]) + b2\n",
    "    a2[i] = sigmoid(z2[i])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or less-awfully, we can vectorize the whole thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Z^{[1]} = W^{[1]}X + b^{[1]}$\n",
    "\n",
    "$A^{[1]} = sigmoid(Z^{[1]})$\n",
    "\n",
    "$Z^{[2]} = W^{[2]}A^{1} + b^{[2]}$\n",
    "\n",
    "$A^{[2]} = sigmoid(Z^{[2]})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "Z1 = np.dot(W1, X) + b\n",
    "A1 = sigmoid(Z1)\n",
    "Z2 = np.dot(W2, A1) + b\n",
    "A2 = sigmoid(Z2)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
